{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMeA+GzFL7Sfn9KeJe+ji1z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ahmetyldrr/Natural-Language-Processing-with-Transformers/blob/main/Ch01_Hello_Transformers_ipynb_adl%C4%B1_not_defterinin_kopyas%C4%B1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://learning.oreilly.com/library/view/natural-language-processing/9781098136789/ch01.html"
      ],
      "metadata": {
        "id": "Y7VRMsmfg2O-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#ðŸ“˜ Chapter 1 â€“ Hello Transformers (AyrÄ±ntÄ±lÄ± AÃ§Ä±klama + Kod + Kaynak)\n",
        "\n",
        "Bu bÃ¶lÃ¼m, Transformer mimarisinin ortaya Ã§Ä±kÄ±ÅŸÄ±nÄ±, neden Ã¶nemli olduÄŸunu, Ã¶nceki yÃ¶ntemlere gÃ¶re avantajlarÄ±nÄ± ve temel yapÄ± taÅŸlarÄ±nÄ± detaylÄ± bir ÅŸekilde ele almaktadÄ±r. AynÄ± zamanda Hugging Face ekosistemi ile bu teknolojileri nasÄ±l kolayca uygulayabileceÄŸimizi gÃ¶sterir.\n",
        "\n",
        "\n",
        "## 1. Transformerâ€™Ä±n Ortaya Ã‡Ä±kÄ±ÅŸÄ±\n",
        "\n",
        "2017 yÄ±lÄ±nda Google araÅŸtÄ±rmacÄ±larÄ± tarafÄ±ndan yayÄ±mlanan â€œAttention Is All You Needâ€ adlÄ± makale, RNN (Recurrent Neural Network) tabanlÄ± sÄ±ralÄ± veri modellemeye karÅŸÄ± bÃ¼yÃ¼k bir alternatif sundu.\n",
        "\n",
        "### âœ”ï¸ Neden Ã¶nemli?\n",
        "- EÄŸitim sÃ¼resi daha kÄ±sa\n",
        "- ParalelleÅŸtirilebilir\n",
        "- Uzun baÄŸlamlarÄ± daha iyi iÅŸler\n",
        "\n",
        "### ðŸ§  Referanslar:\n",
        "- https://arxiv.org/abs/1706.03762\n",
        "- https://jalammar.github.io/illustrated-transformer/\n",
        "- https://sebastianruder.com/the-transformer-family/\n",
        "\n",
        "---\n",
        "\n",
        "## 2. ULMFiT ve Transfer Ã–ÄŸrenmenin GÃ¼cÃ¼\n",
        "\n",
        "ULMFiT, LSTM temelli bir model ile transfer Ã¶ÄŸrenmeyi NLP'ye taÅŸÄ±dÄ±. Bu, Transformer gibi modellerin daha az veriyle Ã§ok ÅŸey Ã¶ÄŸrenebilmesinin yolunu aÃ§tÄ±.\n",
        "\n",
        "### ðŸ§  Kaynak:\n",
        "- https://arxiv.org/abs/1801.06146\n",
        "- https://docs.fast.ai/text.learner.html\n",
        "\n",
        "---\n",
        "\n",
        "## 3. GPT ve BERT'in DoÄŸuÅŸu\n",
        "\n",
        "- **GPT:** Sadece decoder katmanÄ± ile Ã§alÄ±ÅŸÄ±r. Language modeling gÃ¶revine odaklanÄ±r.\n",
        "- **BERT:** Sadece encoder katmanÄ± ile Ã§alÄ±ÅŸÄ±r. Masked Language Modeling kullanÄ±r.\n",
        "\n",
        "### ðŸ§ª Basit Ã–rnek:\n",
        "```python\n",
        "from transformers import pipeline\n",
        "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
        "print(generator(\"Transformers are\", max_length=20))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Encoder-Decoder YapÄ±sÄ±\n",
        "\n",
        "Encoder giriÅŸi temsile Ã§evirir, decoder bu temsilden Ã§Ä±ktÄ± Ã¼retir.\n",
        "\n",
        "### ðŸ§  Okuma:\n",
        "- https://machinelearningmastery.com/encoder-decoder-attention-sequence-to-sequence-prediction-keras/\n",
        "- https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Attention MekanizmasÄ±\n",
        "\n",
        "Attention, decoderâ€™Ä±n her giriÅŸ tokenâ€™Ä±na farklÄ± aÄŸÄ±rlÄ±k vermesini saÄŸlar. Bu sayede daha iyi Ã§eviri ve Ã¶zetleme yapÄ±lÄ±r.\n",
        "\n",
        "### ðŸ§  Kaynaklar:\n",
        "- https://arxiv.org/abs/1409.0473\n",
        "- https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Transformer Mimarisi\n",
        "\n",
        "Transformer mimarisi, katmanlÄ± self-attention ve feedforward yapÄ±larÄ±ndan oluÅŸur. Encoder ve decoder bloklarÄ± ayrÄ± ayrÄ± attention katmanlarÄ± iÃ§erir.\n",
        "\n",
        "```python\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 7. Hugging Face Transformers KÃ¼tÃ¼phanesi\n",
        "\n",
        "Bu kÃ¼tÃ¼phane, model yÃ¼kleme, Ã¶n iÅŸleme, eÄŸitim ve ince ayar gibi iÅŸleri bÃ¼yÃ¼k Ã¶lÃ§Ã¼de kolaylaÅŸtÄ±rÄ±r.\n",
        "\n",
        "### ðŸ“¦ Kurulum:\n",
        "```bash\n",
        "pip install transformers\n",
        "```\n",
        "\n",
        "### ðŸ‘‡ Text Classification Pipeline:\n",
        "```python\n",
        "from transformers import pipeline\n",
        "classifier = pipeline(\"text-classification\")\n",
        "print(classifier(\"This product is awesome!\"))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 8. Uygulama Ã–rnekleri\n",
        "\n",
        "### âœ… Named Entity Recognition (NER)\n",
        "```python\n",
        "ner = pipeline(\"ner\", aggregation_strategy=\"simple\")\n",
        "ner(\"Barack Obama was born in Hawaii.\")\n",
        "```\n",
        "\n",
        "### âœ… Soru Cevaplama\n",
        "```python\n",
        "qa = pipeline(\"question-answering\")\n",
        "qa(question=\"Who founded Apple?\", context=\"Apple was founded by Steve Jobs and Steve Wozniak.\")\n",
        "```\n",
        "\n",
        "### âœ… Ã–zetleme (Summarization)\n",
        "```python\n",
        "summarizer = pipeline(\"summarization\")\n",
        "summarizer(\"Transformers are the backbone of modern NLP.\")\n",
        "```\n",
        "\n",
        "### âœ… Ã‡eviri (Translation)\n",
        "```python\n",
        "translator = pipeline(\"translation_en_to_de\", model=\"Helsinki-NLP/opus-mt-en-de\")\n",
        "translator(\"Hello, how are you?\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 9. Hugging Face Ekosistemi\n",
        "\n",
        "### ðŸš€ Ana BileÅŸenler:\n",
        "- **Transformers:** Modellerin kalbi\n",
        "- **Tokenizers:** HÄ±zlÄ± tokenizasyon\n",
        "- **Datasets:** BÃ¼yÃ¼k veri kÃ¼meleri\n",
        "- **Accelerate:** EÄŸitim kolaylaÅŸtÄ±rÄ±cÄ±\n",
        "- **Hub:** Modellerin ve veri kÃ¼melerinin merkezi\n",
        "\n",
        "### ðŸ”— BaÄŸlantÄ±lar:\n",
        "- https://huggingface.co/models\n",
        "- https://huggingface.co/datasets\n",
        "- https://huggingface.co/docs/transformers/index\n",
        "\n",
        "---\n",
        "\n",
        "## 10. Transformerâ€™larÄ±n ZorluklarÄ±\n",
        "\n",
        "- ðŸ” Uzun dokÃ¼manlarda self-attention maliyetlidir\n",
        "- ðŸ” AÃ§Ä±klanabilirlik problemi\n",
        "- âš–ï¸ Ã–nyargÄ± (bias) riski\n",
        "- ðŸŒ DÃ¼ÅŸÃ¼k kaynaklÄ± diller iÃ§in model eksikliÄŸi\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”š SonuÃ§ ve Sonraki AdÄ±mlar\n",
        "\n",
        "Transformer mimarisi, doÄŸal dil iÅŸleme alanÄ±nda ezber bozdu. Hugging Face ekosistemi sayesinde bu teknolojiler artÄ±k herkes iÃ§in ulaÅŸÄ±labilir. Bu bÃ¶lÃ¼mde teoriden pratiÄŸe geÃ§en temel konularÄ± iÅŸledik.\n",
        "\n",
        "> ðŸ”œ Chapter 2â€™de metin sÄ±nÄ±flandÄ±rmayÄ± derinlemesine iÅŸleyeceÄŸiz!\n",
        "\n"
      ],
      "metadata": {
        "id": "3Sb6auOQ8V88"
      }
    }
  ]
}