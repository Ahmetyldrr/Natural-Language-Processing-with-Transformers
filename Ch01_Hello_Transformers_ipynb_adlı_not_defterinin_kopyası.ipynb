{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMeA+GzFL7Sfn9KeJe+ji1z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ahmetyldrr/Natural-Language-Processing-with-Transformers/blob/main/Ch01_Hello_Transformers_ipynb_adl%C4%B1_not_defterinin_kopyas%C4%B1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://learning.oreilly.com/library/view/natural-language-processing/9781098136789/ch01.html"
      ],
      "metadata": {
        "id": "Y7VRMsmfg2O-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#📘 Chapter 1 – Hello Transformers (Ayrıntılı Açıklama + Kod + Kaynak)\n",
        "\n",
        "Bu bölüm, Transformer mimarisinin ortaya çıkışını, neden önemli olduğunu, önceki yöntemlere göre avantajlarını ve temel yapı taşlarını detaylı bir şekilde ele almaktadır. Aynı zamanda Hugging Face ekosistemi ile bu teknolojileri nasıl kolayca uygulayabileceğimizi gösterir.\n",
        "\n",
        "\n",
        "## 1. Transformer’ın Ortaya Çıkışı\n",
        "\n",
        "2017 yılında Google araştırmacıları tarafından yayımlanan “Attention Is All You Need” adlı makale, RNN (Recurrent Neural Network) tabanlı sıralı veri modellemeye karşı büyük bir alternatif sundu.\n",
        "\n",
        "### ✔️ Neden önemli?\n",
        "- Eğitim süresi daha kısa\n",
        "- Paralelleştirilebilir\n",
        "- Uzun bağlamları daha iyi işler\n",
        "\n",
        "### 🧠 Referanslar:\n",
        "- https://arxiv.org/abs/1706.03762\n",
        "- https://jalammar.github.io/illustrated-transformer/\n",
        "- https://sebastianruder.com/the-transformer-family/\n",
        "\n",
        "---\n",
        "\n",
        "## 2. ULMFiT ve Transfer Öğrenmenin Gücü\n",
        "\n",
        "ULMFiT, LSTM temelli bir model ile transfer öğrenmeyi NLP'ye taşıdı. Bu, Transformer gibi modellerin daha az veriyle çok şey öğrenebilmesinin yolunu açtı.\n",
        "\n",
        "### 🧠 Kaynak:\n",
        "- https://arxiv.org/abs/1801.06146\n",
        "- https://docs.fast.ai/text.learner.html\n",
        "\n",
        "---\n",
        "\n",
        "## 3. GPT ve BERT'in Doğuşu\n",
        "\n",
        "- **GPT:** Sadece decoder katmanı ile çalışır. Language modeling görevine odaklanır.\n",
        "- **BERT:** Sadece encoder katmanı ile çalışır. Masked Language Modeling kullanır.\n",
        "\n",
        "### 🧪 Basit Örnek:\n",
        "```python\n",
        "from transformers import pipeline\n",
        "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
        "print(generator(\"Transformers are\", max_length=20))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Encoder-Decoder Yapısı\n",
        "\n",
        "Encoder girişi temsile çevirir, decoder bu temsilden çıktı üretir.\n",
        "\n",
        "### 🧠 Okuma:\n",
        "- https://machinelearningmastery.com/encoder-decoder-attention-sequence-to-sequence-prediction-keras/\n",
        "- https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Attention Mekanizması\n",
        "\n",
        "Attention, decoder’ın her giriş token’ına farklı ağırlık vermesini sağlar. Bu sayede daha iyi çeviri ve özetleme yapılır.\n",
        "\n",
        "### 🧠 Kaynaklar:\n",
        "- https://arxiv.org/abs/1409.0473\n",
        "- https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Transformer Mimarisi\n",
        "\n",
        "Transformer mimarisi, katmanlı self-attention ve feedforward yapılarından oluşur. Encoder ve decoder blokları ayrı ayrı attention katmanları içerir.\n",
        "\n",
        "```python\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 7. Hugging Face Transformers Kütüphanesi\n",
        "\n",
        "Bu kütüphane, model yükleme, ön işleme, eğitim ve ince ayar gibi işleri büyük ölçüde kolaylaştırır.\n",
        "\n",
        "### 📦 Kurulum:\n",
        "```bash\n",
        "pip install transformers\n",
        "```\n",
        "\n",
        "### 👇 Text Classification Pipeline:\n",
        "```python\n",
        "from transformers import pipeline\n",
        "classifier = pipeline(\"text-classification\")\n",
        "print(classifier(\"This product is awesome!\"))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 8. Uygulama Örnekleri\n",
        "\n",
        "### ✅ Named Entity Recognition (NER)\n",
        "```python\n",
        "ner = pipeline(\"ner\", aggregation_strategy=\"simple\")\n",
        "ner(\"Barack Obama was born in Hawaii.\")\n",
        "```\n",
        "\n",
        "### ✅ Soru Cevaplama\n",
        "```python\n",
        "qa = pipeline(\"question-answering\")\n",
        "qa(question=\"Who founded Apple?\", context=\"Apple was founded by Steve Jobs and Steve Wozniak.\")\n",
        "```\n",
        "\n",
        "### ✅ Özetleme (Summarization)\n",
        "```python\n",
        "summarizer = pipeline(\"summarization\")\n",
        "summarizer(\"Transformers are the backbone of modern NLP.\")\n",
        "```\n",
        "\n",
        "### ✅ Çeviri (Translation)\n",
        "```python\n",
        "translator = pipeline(\"translation_en_to_de\", model=\"Helsinki-NLP/opus-mt-en-de\")\n",
        "translator(\"Hello, how are you?\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 9. Hugging Face Ekosistemi\n",
        "\n",
        "### 🚀 Ana Bileşenler:\n",
        "- **Transformers:** Modellerin kalbi\n",
        "- **Tokenizers:** Hızlı tokenizasyon\n",
        "- **Datasets:** Büyük veri kümeleri\n",
        "- **Accelerate:** Eğitim kolaylaştırıcı\n",
        "- **Hub:** Modellerin ve veri kümelerinin merkezi\n",
        "\n",
        "### 🔗 Bağlantılar:\n",
        "- https://huggingface.co/models\n",
        "- https://huggingface.co/datasets\n",
        "- https://huggingface.co/docs/transformers/index\n",
        "\n",
        "---\n",
        "\n",
        "## 10. Transformer’ların Zorlukları\n",
        "\n",
        "- 🔁 Uzun dokümanlarda self-attention maliyetlidir\n",
        "- 🔍 Açıklanabilirlik problemi\n",
        "- ⚖️ Önyargı (bias) riski\n",
        "- 🌍 Düşük kaynaklı diller için model eksikliği\n",
        "\n",
        "---\n",
        "\n",
        "## 🔚 Sonuç ve Sonraki Adımlar\n",
        "\n",
        "Transformer mimarisi, doğal dil işleme alanında ezber bozdu. Hugging Face ekosistemi sayesinde bu teknolojiler artık herkes için ulaşılabilir. Bu bölümde teoriden pratiğe geçen temel konuları işledik.\n",
        "\n",
        "> 🔜 Chapter 2’de metin sınıflandırmayı derinlemesine işleyeceğiz!\n",
        "\n"
      ],
      "metadata": {
        "id": "3Sb6auOQ8V88"
      }
    }
  ]
}