# Kaynakça ve Kullanılan Materyaller

Aşağıda belirtilen kaynaklar kullanılmıştır:

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin , 2017, Dikkat Her Şeydir : https://arxiv.org/abs/1706.03762 

Hugging Face transformer kullanımı: https://huggingface.co/docs/transformers/main/en/quicktour 

Tensor2Tensor ( T2T ) tanıtımı: https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb?hl=en 

Manuel Romero'nun Raimi Karim tarafından yapılan açıklamalara bağlantı içeren not defteri : https://colab.research.google.com/drive/1rPk3ohrmVclqhH7uQ7qys4oznDdAhpzF 

Google dil araştırmaları: https://research.google/teams/language / 

Hugging Face araştırmaları: https://huggingface.co/transformers/index.html 

The Annotated Transformer : http://nlp.seas.harvard.edu/2018/04/03/attention.html 

Jay Alammar , The Illustrated Transformer : http://jalammar.github.io/illustrated-transformer/

Paragrafta herhangi bir python kodu bulunmamaktadır. Sadece kaynakça listelenmiştir. 

Eğer bir python kodu olsaydı, kodu çalıştırarak veya kodun her bir satırını açıklayarak anlatırdım. Ancak burada sadece çeviri işlemi gerçekleştirilmiştir.

---

