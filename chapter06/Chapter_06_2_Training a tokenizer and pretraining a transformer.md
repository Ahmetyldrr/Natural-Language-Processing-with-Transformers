# Bu Bölümde Yapılacaklar
Bu bölümde, Hugging Face tarafından BERT benzeri modeller için sağlanan yapı taşlarını kullanarak KantaiBERT adlı bir transformer modeli eğiteceğiz. Kullanacağımız modelin yapı taşlarının teorisini 5. Bölüm, BERT aracılığıyla İnce Ayarlama'ya Dalışta ele aldık. Önceki bölümlerde edindiğimiz bilgilere dayanarak KantaiBERT'i tanımlayacağız. KantaiBERT, Gelişmiş Bir Şekilde Optimize Edilmiş BERT Ön Eğitim Yaklaşımı (RoBERTa) benzeri bir modeldir. RoBERTa, Meta (eski adıyla Facebook) tarafından tasarlanan gelişmiş bir BERT sürümüdür. İlk BERT modelleri, 5. Bölüm, BERT aracılığıyla İnce Ayarlama'ya Dalışta gördüğümüz gibi, ilk transformer modellerine yenilikçi özellikler getirdi. RoBERTa, ön eğitim sürecinin mekaniğini geliştirerek downstream görevleri için transformerların performansını artırır. Örneğin, WordPiece tokenization kullanmaz, bunun yerine bayt düzeyinde Bayt Çifti Kodlaması (BPE) kullanır. Bu yöntem, çok çeşitli BERT ve BERT benzeri modellerin yolunu açtı. Bu bölümde, KantaiBERT, BERT gibi, Maskeli Dil Modelleme (MLM) kullanılarak eğitilecektir. MLM, bir dizideki bir kelimeyi maskeleyen bir dil modelleme tekniğidir. Ardından, transformer modeli maskeli kelimeyi tahmin etmek için eğitilmelidir. KantaiBERT, 6 katman, 12 kafa ve 83.504.416 parametre ile küçük bir model olarak eğitilecektir (bu sayı model güncellendiğinde değişebilir). 83 milyon parametrenin çok görünebileceği düşünülebilir. Ancak, parametreler 12 kafa üzerine yayılmıştır, bu da onu nispeten küçük bir model yapar. Küçük bir model, ön eğitim deneyimini sorunsuz hale getirecektir, böylece her adım gerçek zamanlı olarak görülebilir, saatlerce sonuç beklemeye gerek kalmadan. KantaiBERT, 6 katman ve 12 kafa kullanan bir DistilBERT (BERT'in destile edilmiş bir sürümü) mimarisi kullanarak RoBERTa'nın daha küçük bir sürümüdür. Bu nedenle, KantaiBERT çok daha hızlı çalışır, ancak sonuçlar tam RoBERTa model konfigürasyonuna göre biraz daha az doğrudur. Büyük modellerin mükemmel performans elde ettiğini biliyoruz. Peki, bir modeli akıllı telefon üzerinde çalıştırmak isterseniz? Miniaturization teknolojik evrimin anahtarı olmuştur. Transformerlar da bazen uygulama sırasında aynı yolu izlemek zorunda kalacaktır. Daha az parametre kullanarak veya gelecekteki diğer yöntemlerle destilasyon, ön eğitimin en iyisini almak ve birçok downstream görevin ihtiyaçları için verimli hale getirmek için akıllıca bir yoldur. Küçük bir modeli akıllı telefon üzerinde çalıştırmak da dahil olmak üzere tüm olası mimarileri göstermek önemlidir. Ancak, transformerların geleceği aynı zamanda hazır kullanımlık API'ler olacaktır, 7. Bölüm, ChatGPT ile Üretken Yapay Zeka Devrimi'nde göreceğimiz gibi. KantaiBERT, GPT-2 tarafından kullanılan gibi bayt düzeyinde bayt çifti kodlaması tokenizer uygulayacaktır. Özel tokenler, RoBERTa tarafından kullanılanlar olacaktır. BERT modelleri çoğunlukla WordPiece tokenizer kullanır. Bir tokenin bir segmentin hangi kısmına ait olduğunu belirtmek için token tipi ID'leri yoktur. Bunun yerine, segmentler </s> ayırma tokeni ile ayrılacaktır. KantaiBERT, özel bir veri kümesi kullanacak, bir tokenizör eğitecek, transformer modelini eğitecek, kaydedecek ve bir MLM örneği ile çalıştıracaktır. Hadi başlayalım ve sıfırdan bir transformer oluşturalım.

Python kodları bulunmamaktadır.

---

