# Google Araştırma Takımı Tarafından Yapılan Optimizasyonlar

Google araştırma takımı, eğitim süreçlerini optimize etti, bu da umut verici bir şekilde donanım gereksinimlerinin boyutunu ve gücünü azaltabilir. Optimal model boyutu (N) ve parametre sayısı (D), FLOPs bütçesi 1x10^19'dan 1x10^22 FLOPs'a arttıkça eşit oranlarda büyüdüğü tespit edildi. Bu görece önemsiz gibi görünse de kritik bir öneme sahip. Biz ölçeklendirme yasalarına tanık oluyoruz. Transformer teknolojisi, deneysel testlerden dersler çıkarmaya ve diğer araştırmacıların da diğer yasaları üretmesine yardımcı olacak yasalar oluşturmaya doğru ilerliyor. PaLM araştırma takımı, 400M'den 15B'ye kadar çeşitli modelleri eğitti. 1x10^22 FLOPs'luk bir model için optimal parametre sayısının sadece 10B olduğunu buldular (Anil ve diğerleri (2023), sayfa 7)! Bu, transformer yazılımı ve donanım optimizasyonuna yönelik etkileyici bir ilerlemedir.

**Paragrafın birebir çevirisi yukarıda verilmiştir. Python kodları bulunmamaktadır.**

Eğer bir python kodu olsaydı, satır satır açıklama aşağıdaki gibi yapılırdı:

Örneğin aşağıdaki python kodu için:

```python
# FLOPs bütçesinin hesaplanması
flops_bütçesi = 1e22

# Optimal parametre sayısının hesaplanması
optimal_parametre_sayisi = 10e9

print("Optimal parametre sayısı:", optimal_parametre_sayisi)
```

*   `# FLOPs bütçesinin hesaplanması`: Bu satır, FLOPs bütçesinin hesaplanmasını açıklar. `flops_bütçesi` değişkenine 1x10^22 değeri atanır.
*   `# Optimal parametre sayısının hesaplanması`: Bu satır, optimal parametre sayısının hesaplanmasını açıklar. `optimal_parametre_sayisi` değişkenine 10B (10e9) değeri atanır.
*   `print("Optimal parametre sayısı:", optimal_parametre_sayisi)`: Bu satır, optimal parametre sayısını yazdırır.

---

