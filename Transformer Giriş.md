
# ğŸ“˜ Chapter 1 â€“ Hello Transformers (GeniÅŸletilmiÅŸ AÃ§Ä±klama + Kod + Kaynak)

Bu bÃ¶lÃ¼mde, Transformer mimarisinin tarihsel geliÅŸimi, neden devrimsel olduÄŸu, Ã¶ncÃ¼llerinden farklarÄ±, uygulama Ã¶rnekleri ve Hugging Face ekosistemi ile nasÄ±l kolayca Ã§alÄ±ÅŸabileceÄŸimiz detaylÄ± olarak ele alÄ±nmÄ±ÅŸtÄ±r.

---

## 1. Transformerâ€™Ä±n Ortaya Ã‡Ä±kÄ±ÅŸÄ±

2017 yÄ±lÄ±nda Google tarafÄ±ndan yayÄ±mlanan â€œAttention Is All You Needâ€ adlÄ± makale, RNN ve LSTM tabanlÄ± sÄ±ralÄ± modellemelere alternatif olarak sadece attention mekanizmasÄ±na dayalÄ± bir yapÄ± sundu.

### âœ”ï¸ Neden Ã¶nemli?
- EÄŸitim sÃ¼resi daha kÄ±sa, daha ucuz GPU maliyeti
- Zaman serilerine gÃ¶re paralel iÅŸlem yapabilir
- Uzun baÄŸlamlarda bilgi kaybÄ± daha az

### ğŸ§  Referanslar:
- [Makale](https://arxiv.org/abs/1706.03762)
- [GÃ¶rselli anlatÄ±m â€“ Jay Alammar](https://jalammar.github.io/illustrated-transformer/)
- [Transformer Ailesi â€“ Ruder](https://sebastianruder.com/the-transformer-family/)

---

## 2. ULMFiT ve Transfer Ã–ÄŸrenme

ULMFiT, LSTM Ã¼zerine kurulu olmasÄ±na raÄŸmen, geniÅŸ Ã§aplÄ± bir gÃ¶zetimsiz eÄŸitim sonrasÄ±, kÃ¼Ã§Ã¼k etiketli veri setlerinde bile Ã§ok baÅŸarÄ±lÄ± sonuÃ§lar elde etmeyi saÄŸlamÄ±ÅŸtÄ±r.

### ğŸ§  Kaynaklar:
- [Makale](https://arxiv.org/abs/1801.06146)
- [FastAI Belgeleri](https://docs.fast.ai/text.learner.html)

Bu yÃ¶ntem, Transformer tabanlÄ± modellerin Ã¶nÃ¼nÃ¼ aÃ§tÄ± Ã§Ã¼nkÃ¼ aynÄ± "Ã¶nce bÃ¼yÃ¼k veriyle eÄŸit, sonra uyum saÄŸla" mantÄ±ÄŸÄ± GPT ve BERT ile kullanÄ±lacaktÄ±r.

---

## 3. GPT ve BERTâ€™in DoÄŸuÅŸu

- **GPT (Generative Pretrained Transformer):** YalnÄ±zca decoder kÄ±smÄ± kullanÄ±lÄ±r. Dil modeli olarak eÄŸitim alÄ±r.
- **BERT (Bidirectional Encoder Representations from Transformers):** YalnÄ±zca encoder kÄ±smÄ± kullanÄ±lÄ±r. CÃ¼mle iÃ§indeki rastgele maskelenmiÅŸ kelimeleri tahmin etmeye Ã§alÄ±ÅŸÄ±r.

### ğŸ§ª GPT2 ile Metin Ãœretimi
```python
from transformers import pipeline
generator = pipeline("text-generation", model="gpt2")
print(generator("Transformers are", max_length=20))
```

---

## 4. Encoder-Decoder YapÄ±sÄ±

Bu yapÄ±, makine Ã§evirisi gibi "girdi-seviyesi dizi â†’ Ã§Ä±ktÄ±-seviyesi dizi" iÅŸlemleri iÃ§in uygundur.

- Encoder, tÃ¼m girdiyi sabit boyutlu bir vektÃ¶re kodlar.
- Decoder, bu vektÃ¶rden anlam Ã§Ä±kararak Ã§Ä±ktÄ± Ã¼retir.

### ğŸ§  Kaynaklar:
- [Sequence-to-Sequence YapÄ±lar](https://machinelearningmastery.com/encoder-decoder-attention-sequence-to-sequence-prediction-keras/)
- [Colahâ€™s LSTM AnlatÄ±mÄ±](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)

---

## 5. Attention MekanizmasÄ±

Attention, modelin farklÄ± giriÅŸ Ã¶ÄŸelerine farklÄ± dikkat vermesini saÄŸlar. Bu, Ã§eviri gibi eÅŸleÅŸme (alignment) gereken gÃ¶revlerde bÃ¼yÃ¼k avantaj saÄŸlar.

### ğŸ§  Kaynaklar:
- [Bahdanau Attention](https://arxiv.org/abs/1409.0473)
- [Jay Alammar GÃ¶rselleÅŸtirme](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)

---

## 6. Transformer Mimarisi

Transformer, encoder ve decoder katmanlarÄ±nÄ±n her birinde self-attention + feed-forward katmanlar bulundurur. Her katman kendi baÅŸÄ±na bir blok gibi Ã§alÄ±ÅŸÄ±r.

### ğŸ”§ Kod:
```python
from transformers import AutoTokenizer, AutoModel
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
model = AutoModel.from_pretrained("bert-base-uncased")
```

---

## 7. Hugging Face Transformers KÃ¼tÃ¼phanesi

Bu Python kÃ¼tÃ¼phanesi, Ã¶nceden eÄŸitilmiÅŸ modellerle Ã§alÄ±ÅŸma ve kendi verinizle model eÄŸitme iÅŸlemlerini Ã§ok kolay hale getirir.

### ğŸ“¦ Kurulum:
```bash
pip install transformers
```

### ğŸ‘‡ Text Classification Pipeline:
```python
from transformers import pipeline
classifier = pipeline("text-classification")
print(classifier("This product is awesome!"))
```

---

## 8. Uygulama Ã–rnekleri

### âœ… Named Entity Recognition (NER)
```python
ner = pipeline("ner", aggregation_strategy="simple")
ner("Barack Obama was born in Hawaii.")
```

### âœ… Soru Cevaplama
```python
qa = pipeline("question-answering")
qa(question="Who founded Apple?", context="Apple was founded by Steve Jobs and Steve Wozniak.")
```

### âœ… Ã–zetleme (Summarization)
```python
summarizer = pipeline("summarization")
summarizer("Transformers are the backbone of modern NLP.")
```

### âœ… Ã‡eviri (Translation)
```python
translator = pipeline("translation_en_to_de", model="Helsinki-NLP/opus-mt-en-de")
translator("Hello, how are you?")
```

---

## 9. Hugging Face Ekosistemi

### ğŸš€ BileÅŸenler:
- **Transformers:** Model eÄŸitimi ve uygulamasÄ±
- **Tokenizers:** Tokenize iÅŸlemi
- **Datasets:** GeniÅŸ veri kÃ¼mesi koleksiyonu
- **Accelerate:** Multi-GPU ve kolay eÄŸitim arayÃ¼zÃ¼
- **Hub:** HazÄ±r modellerin merkezi

### ğŸ”— BaÄŸlantÄ±lar:
- https://huggingface.co/models
- https://huggingface.co/datasets
- https://huggingface.co/docs/transformers/index

---

## 10. Transformerâ€™larÄ±n ZorluklarÄ±

- ğŸ” Uzun dokÃ¼manlarda performans problemi (quadratic attention)
- ğŸ” AÃ§Ä±klanabilirlik hÃ¢lÃ¢ sÄ±nÄ±rlÄ±
- âš–ï¸ Modelin eÄŸitildiÄŸi verilerdeki Ã¶nyargÄ±lar (bias) modele yansÄ±yabilir
- ğŸŒ DÃ¼ÅŸÃ¼k kaynaklÄ± dillerde veri azlÄ±ÄŸÄ±

---

## ğŸ”š SonuÃ§ ve Sonraki AdÄ±mlar

Bu bÃ¶lÃ¼mde:
- Transformer mimarisinin nasÄ±l ortaya Ã§Ä±ktÄ±ÄŸÄ±
- GPT ve BERT gibi modellerin nasÄ±l bu yapÄ±ya dayandÄ±ÄŸÄ±
- Hugging Face ile pratikte nasÄ±l kullanÄ±lacaÄŸÄ±
- Kod Ã¶rnekleri ve kaynaklarla birlikte Ã¶ÄŸrenildi.

> ğŸš€ Devam etmek iÃ§in: Chapter 2 â€“ Text Classification!


