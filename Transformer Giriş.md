
## ğŸ“š Chapter 1 â€“ Hello Transformers: DetaylÄ± Notlar ve Kodlar

### 1. Transformerâ€™Ä±n DoÄŸuÅŸu
- 2017 yÄ±lÄ±nda Google tarafÄ±ndan yayÄ±nlanan "Attention Is All You Need" makalesiyle transformer mimarisi tanÄ±tÄ±ldÄ±.
- Bu model, RNN (Recurrent Neural Networks) yapÄ±sÄ±ndan farklÄ± olarak tamamen attention mekanizmasÄ±na dayalÄ±ydÄ±.
- RNN'lerin sÄ±ralÄ± iÅŸleme zorunluluÄŸuna karÅŸÄ±lÄ±k, transformer paralel Ã§alÄ±ÅŸabilir ve daha hÄ±zlÄ±dÄ±r.

### 2. GPT ve BERTâ€™in Ortaya Ã‡Ä±kÄ±ÅŸÄ±
- GPT (Generative Pretrained Transformer): Decoder tarafÄ±nÄ± kullanÄ±r, "language modeling" yapar.
- BERT (Bidirectional Encoder Representations from Transformers): Encoder yapÄ±sÄ±nÄ± kullanÄ±r, "masked language modeling" uygular.
- Her ikisi de transfer learning kullanarak farklÄ± NLP gÃ¶revlerinde state-of-the-art sonuÃ§lar verdi.

### 3. Encoder-Decoder Framework
- Encoder, giriÅŸ metnini bir gizli temsil (hidden state) haline getirir.
- Decoder bu temsili kullanarak Ã§Ä±ktÄ± metnini oluÅŸturur.
- Ã–rnek kod (RNN yerine transformer olmasÄ±nÄ± temsilen):
```python
from transformers import MarianMTModel, MarianTokenizer
model_name = "Helsinki-NLP/opus-mt-en-de"
tokenizer = MarianTokenizer.from_pretrained(model_name)
model = MarianMTModel.from_pretrained(model_name)
```

### 4. Attention Mechanism
- Decoder, encoderâ€™Ä±n her adÄ±mdaki gizli durumuna eriÅŸerek daha doÄŸru Ã§eviriler yapabilir.
- Attention, her encoder Ã§Ä±ktÄ±sÄ±na aÄŸÄ±rlÄ±k vererek hangi kelimenin daha Ã¶nemli olduÄŸunu belirler.
- Visualization Ã§ok kullanÄ±lÄ±r: "alignment heatmaps"

### 5. Transformer Mimarisi
- RNN yok, tamamen self-attention katmanlarÄ± var.
- Encoder ve decoder kendi iÃ§inde attention uygular.
- Paralel Ã§alÄ±ÅŸabilir, bu da eÄŸitimi Ã§ok daha hÄ±zlÄ± hale getirir.

### 6. Transfer Learning in NLP
- ULMFiT: Ä°lk olarak bÃ¼yÃ¼k corpus ile pretraining, sonra hedef domain'e (IMDb vs.) adaptasyon.
- Son adÄ±mda ince ayar (fine-tuning) ile hedef gÃ¶reve uyarlama.
- Ã–rnek:
```python
from transformers import AutoModelForSequenceClassification, AutoTokenizer
model = AutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased")
tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")
```

### 7. Hugging Face Transformers KÃ¼tÃ¼phanesi
- PyTorch, TensorFlow ve JAX uyumlu
- Pipeline API ile kullanÄ±m Ã§ok kolay
- Model, tokenizer ve pipeline tek satÄ±rda Ã§alÄ±ÅŸtÄ±rÄ±labilir
- Ã–rnek:
```python
from transformers import pipeline
classifier = pipeline("text-classification")
```

### 8. Uygulama Ã–rnekleri (Pipeline API ile)

#### âœ”ï¸ Text Classification
```python
classifier = pipeline("text-classification")
print(classifier("I love Hugging Face Transformers!"))
```

#### âœ”ï¸ Named Entity Recognition (NER)
```python
ner = pipeline("ner", aggregation_strategy="simple")
print(ner("Barack Obama was born in Hawaii."))
```

#### âœ”ï¸ Question Answering
```python
qa = pipeline("question-answering")
qa(question="Who founded Apple?", context="Apple was founded by Steve Jobs and Steve Wozniak.")
```

#### âœ”ï¸ Summarization
```python
summarizer = pipeline("summarization")
text = "Transformers allow parallel computation. They work better than RNNs in many NLP tasks."
summarizer(text)
```

#### âœ”ï¸ Translation
```python
translator = pipeline("translation_en_to_de", model="Helsinki-NLP/opus-mt-en-de")
print(translator("Hello, how are you?"))
```

#### âœ”ï¸ Text Generation
```python
generator = pipeline("text-generation")
prompt = "Once upon a time"
print(generator(prompt, max_length=50))
```

### 9. Hugging Face Ekosistemi
- **Transformers**: modeller
- **Tokenizers**: hÄ±zlÄ± metin bÃ¶lÃ¼leyici
- **Datasets**: veri yÃ¼kleme, Ã¶niÅŸleme
- **Accelerate**: GPU/Ã§oklu cihaz eÄŸitimi
- **Hub**: 20.000+ model/dataset merkezi

### 10. Transformer KullanÄ±mÄ±ndaki Zorluklar
- Ä°ngilizce dÄ±ÅŸÄ± dillerde veri/model azlÄ±ÄŸÄ±
- Uzun belgelerde self-attention maliyeti
- Modelin neden belirli sonuÃ§lar verdiÄŸi anlaÅŸÄ±lamaz (black box)
- Ä°nternetteki verilerden gelen bias etkisi

### 11. BÃ¶lÃ¼m Ã–zeti ve Gelecek AÅŸama
- Transformer mimarisi, attention mekanizmasÄ±, encoder-decoder yapÄ±sÄ± tanÄ±ndÄ±
- Hugging Face ile pratik yapÄ±ldÄ±
- Bir sonraki adÄ±m: **kendi verinle model fine-tune etmek (Text Classification)**
