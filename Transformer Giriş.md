
# ğŸ“š Chapter 1 â€“ Hello Transformers (DetaylÄ± AÃ§Ä±klamalar ve Kodlarla)

## 1. Transformerâ€™Ä±n DoÄŸuÅŸu
2017â€™de Googleâ€™Ä±n yayÄ±mladÄ±ÄŸÄ± â€œAttention Is All You Needâ€ makalesiyle Transformer mimarisi tanÄ±tÄ±ldÄ±. Bu yapÄ± RNNâ€™lere gÃ¶re Ã§ok daha hÄ±zlÄ± ve paralel Ã§alÄ±ÅŸabiliyor.

**Avantajlar:**
- Paralel hesaplama
- Daha uzun baÄŸlam bilgisi
- Daha hÄ±zlÄ± eÄŸitim

![Transformer Timeline](https://jalammar.github.io/images/t/transformers-timeline.png)  
*Transformer modellerinin geliÅŸim sÃ¼reci*

---

## 2. GPT ve BERTâ€™in Ortaya Ã‡Ä±kÄ±ÅŸÄ±

| Model | Mimarisi | YÃ¶nlÃ¼lÃ¼k | EÄŸitim TÃ¼rÃ¼ |
|-------|----------|----------|--------------|
| GPT   | Decoder | Tek yÃ¶nlÃ¼ | Language Modeling |
| BERT  | Encoder | Ã‡ift yÃ¶nlÃ¼ | Masked Language Modeling |

```python
from transformers import pipeline
generator = pipeline("text-generation", model="gpt2")
print(generator("Transformers are", max_length=20))
```

---

## 3. Encoder-Decoder Framework

Makine Ã§evirisi gibi gÃ¶revlerde kullanÄ±lÄ±r. Encoder giriÅŸ metnini sayÄ±sal forma Ã§evirir, decoder Ã§Ä±ktÄ±yÄ± Ã¼retir.

```python
from transformers import MarianMTModel, MarianTokenizer

model_name = "Helsinki-NLP/opus-mt-en-de"
tokenizer = MarianTokenizer.from_pretrained(model_name)
model = MarianMTModel.from_pretrained(model_name)

inputs = tokenizer("Hello world", return_tensors="pt")
translated = model.generate(**inputs)
print(tokenizer.decode(translated[0], skip_special_tokens=True))
```

![Encoder Decoder](https://jalammar.github.io/images/seq2seq_2.png)

---

## 4. Attention MekanizmasÄ±

Attention, modelin her kelimeye farklÄ± odaklanmasÄ±nÄ± saÄŸlar.

![Attention](https://jalammar.github.io/images/transformer/transformer-self-attention.png)

```python
# Bu gÃ¶rseldeki yapÄ± transformer mimarisinin temelidir
```

---

## 5. Transformer Mimarisi

Transformer, self-attention + feed-forward aÄŸlardan oluÅŸur. Katman katman iÅŸlemler yapÄ±lÄ±r.

![Transformer Architecture](https://jalammar.github.io/images/transformer/transformer.png)

---

## 6. Transfer Learning in NLP

BÃ¼yÃ¼k veri ile Ã¶n eÄŸitim yapÄ±ldÄ±ktan sonra kÃ¼Ã§Ã¼k gÃ¶rev verileriyle ince ayar yapÄ±lÄ±r (fine-tuning).

```python
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased")
tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")
```

---

## 7. Hugging Face Transformers KÃ¼tÃ¼phanesi

```python
from transformers import pipeline
classifier = pipeline("text-classification")
print(classifier("Transformers are amazing!"))
```

---

## 8. Uygulama Ã–rnekleri

### ğŸ§  Sentiment Analysis
```python
classifier("I hate waiting for packages.")
```

### ğŸ” Named Entity Recognition
```python
ner = pipeline("ner", aggregation_strategy="simple")
ner("Elon Musk founded SpaceX in California.")
```

### â“ Question Answering
```python
qa = pipeline("question-answering")
qa(question="Who founded Apple?", context="Apple was founded by Steve Jobs and Steve Wozniak.")
```

### âœ‚ï¸ Summarization
```python
summarizer = pipeline("summarization")
summarizer("Transformers are powerful NLP models. They outperform RNNs in many tasks.")
```

---

## 9. Hugging Face Ekosistemi

- ğŸ¤– **Transformers**: modeller
- âœ‚ï¸ **Tokenizers**: hÄ±zlÄ± parÃ§alama
- ğŸ§º **Datasets**: veri setleri
- ğŸš€ **Accelerate**: eÄŸitim hÄ±zlandÄ±rma
- â˜ï¸ **Hub**: model havuzu

---

## 10. Zorluklar

- Ä°ngilizce dÄ±ÅŸÄ± dillerde veri sÄ±kÄ±ntÄ±sÄ±
- Uzun dokÃ¼manlarda performans kaybÄ±
- Model iÃ§ yapÄ±sÄ±nÄ±n aÃ§Ä±klanamamasÄ±
- Ã–nyargÄ± (bias) taÅŸÄ±ma riski

---

## 11. BÃ¶lÃ¼m Ã–zeti

Transformer mimarisi NLPâ€™de devrim yaratmÄ±ÅŸtÄ±r. GPT ve BERT ile baÅŸlayan bu Ã§aÄŸ, Hugging Face sayesinde pratik olarak eriÅŸilebilir hale gelmiÅŸtir.

---

ğŸ“Œ *Not: GÃ¶rseller [Jay Alammar](https://jalammar.github.io/illustrated-transformer/) tarafÄ±ndan hazÄ±rlanmÄ±ÅŸtÄ±r.*
