
# ğŸ“š Chapter 1 â€“ Hello Transformers (GeniÅŸletilmiÅŸ AnlatÄ±m + Kaynak Linkleri)

## 1. Transformerâ€™Ä±n DoÄŸuÅŸu

2017 yÄ±lÄ±nda Google araÅŸtÄ±rmacÄ±larÄ± tarafÄ±ndan yayÄ±mlanan â€œAttention Is All You Needâ€ makalesiyle birlikte, RNN tabanlÄ± sÄ±ralÄ± modellemeler yerini paralel Ã§alÄ±ÅŸan Transformer mimarisine bÄ±raktÄ±. Bu mimari, NLP'de devrim yarattÄ±.

### ğŸ§  Temel Kavramlar:
- Paralel hesaplama yeteneÄŸi
- Self-Attention mekanizmasÄ± ile baÄŸlamsal Ã¶ÄŸrenme
- RNN'lerin sÄ±ralÄ± baÄŸÄ±mlÄ±lÄ±klarÄ±nÄ±n kaldÄ±rÄ±lmasÄ±

### ğŸ”— Kaynaklar:
- [Original Paper â€“ Attention Is All You Need (arXiv)](https://arxiv.org/abs/1706.03762)
- [Jay Alammar â€“ Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)
- [Sebastian Ruder â€“ The Transformer Family](https://sebastianruder.com/the-transformer-family/)

---

## 2. GPT ve BERTâ€™in Ortaya Ã‡Ä±kÄ±ÅŸÄ±

GPT ve BERT, Transformer mimarisinin iki farklÄ± yÃ¶nÃ¼nÃ¼ kullanarak NLP gÃ¶revlerine odaklanÄ±r.

| Model | KullanÄ±lan Katman | EÄŸitim TÃ¼rÃ¼ | YÃ¶nlÃ¼lÃ¼k |
|-------|-------------------|-------------|----------|
| GPT   | Decoder           | Language Modeling | Tek yÃ¶nlÃ¼ |
| BERT  | Encoder           | Masked Language Modeling | Ã‡ift yÃ¶nlÃ¼ |

### ğŸ§ª Ã–rnek Kod (GPT):
```python
from transformers import pipeline
generator = pipeline("text-generation", model="gpt2")
print(generator("Transformers are", max_length=20))
```

### ğŸ”— Kaynaklar:
- [BERT: Pre-training of Deep Bidirectional Transformers](https://arxiv.org/abs/1810.04805)
- [OpenAI GPT Paper](https://openai.com/research/language-unsupervised)
- [Blog: BERT vs GPT](https://towardsdatascience.com/bert-vs-gpt-comparing-their-strengths-634f5b40e49e)

---

## 3. Encoder-Decoder Framework

Encoder-Decoder mimarisi, Ã§eviri gibi giriÅŸ-Ã§Ä±kÄ±ÅŸ Ã§iftleri olan gÃ¶revlerde yaygÄ±ndÄ±r. Encoder tÃ¼m giriÅŸi temsil eden bir vektÃ¶re Ã§evirir, decoder bu vektÃ¶rÃ¼ kullanarak Ã§Ä±ktÄ± Ã¼retir.

### ğŸ§ª Ã–rnek Kod:
```python
from transformers import MarianMTModel, MarianTokenizer
model_name = "Helsinki-NLP/opus-mt-en-de"
tokenizer = MarianTokenizer.from_pretrained(model_name)
model = MarianMTModel.from_pretrained(model_name)

inputs = tokenizer("Hello world", return_tensors="pt")
translated = model.generate(**inputs)
print(tokenizer.decode(translated[0], skip_special_tokens=True))
```

### ğŸ”— Kaynaklar:
- [Sequence to Sequence Models (Colah Blog)](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)
- [Encoder-Decoder in Machine Translation](https://machinelearningmastery.com/encoder-decoder-attention-sequence-to-sequence-prediction-keras/)

---

## 4. Attention MekanizmasÄ±

Attention, modelin her kelimeye farklÄ± dikkat vermesini saÄŸlar. Bu sayede baÄŸlam bilgisini daha iyi yakalayabilir.

### ğŸ”— Kaynaklar:
- [Bahdanau Attention Paper](https://arxiv.org/abs/1409.0473)
- [The Illustrated Transformer (Jay Alammar)](https://jalammar.github.io/illustrated-transformer/)
- [Blog: Visualizing Attention](https://distill.pub/2016/augmented-rnns/)

---

## 5. Transformer Mimarisi

Transformer, Ã§ok katmanlÄ± attention ve feed-forward aÄŸlardan oluÅŸur. EÄŸitim sÃ¼resi kÄ±salÄ±r, paralelleÅŸtirme yapÄ±labilir.

### ğŸ”— Kaynaklar:
- [Google AI Blog on Transformer](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html)
- [Blog: A Visual Guide to Transformers](https://e2eml.school/transformers.html)

---

## 6. Transfer Learning in NLP

Transfer Ã¶ÄŸrenme sayesinde bÃ¼yÃ¼k veriyle eÄŸitilmiÅŸ modeller, yeni gÃ¶revlerde Ã§ok daha az veriyle baÅŸarÄ±lÄ± olur. ULMFiT ve BERT bu stratejiyle eÄŸitildi.

### ğŸ§ª Hugging Face Fine-tuning:
```python
from transformers import AutoModelForSequenceClassification, AutoTokenizer
model = AutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased")
tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")
```

### ğŸ”— Kaynaklar:
- [ULMFiT Paper](https://arxiv.org/abs/1801.06146)
- [Transfer Learning with Transformers](https://ruder.io/nlp-imagenet/)
- [Fast.ai ULMFiT Explanation](https://docs.fast.ai/text.learner.html)

---

## 7. Hugging Face Transformers KÃ¼tÃ¼phanesi

Hugging Face, model ve tokenizer yÃ¼kleme, pipeline oluÅŸturma gibi iÅŸleri kolaylaÅŸtÄ±rÄ±r.

### ğŸ§ª Ã–rnek:
```python
from transformers import pipeline
classifier = pipeline("text-classification")
print(classifier("Transformers are amazing!"))
```

### ğŸ”— Kaynaklar:
- [Transformers GitHub](https://github.com/huggingface/transformers)
- [Hugging Face Documentation](https://huggingface.co/docs/transformers/index)

---

## 8. Uygulama Ã–rnekleri

### a. Sentiment Analysis
```python
pipeline("text-classification")("I love AI!")
```

### b. Named Entity Recognition
```python
pipeline("ner", aggregation_strategy="simple")("Barack Obama was born in Hawaii.")
```

### c. Question Answering
```python
pipeline("question-answering")(
  question="Who founded Apple?", 
  context="Apple was founded by Steve Jobs and Steve Wozniak."
)
```

### d. Summarization
```python
pipeline("summarization")("Transformers allow parallel computation and are powerful.")
```

### e. Translation
```python
pipeline("translation_en_to_de", model="Helsinki-NLP/opus-mt-en-de")("Hello, how are you?")
```

### f. Text Generation
```python
pipeline("text-generation", model="gpt2")("Once upon a time")
```

### ğŸ”— Ã–rnek Projeler:
- [Text Classification Example](https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-classification)
- [NER Example](https://github.com/huggingface/transformers/tree/main/examples/pytorch/token-classification)
- [Summarization Example](https://github.com/huggingface/transformers/tree/main/examples/pytorch/summarization)
- [Translation Example](https://github.com/huggingface/transformers/tree/main/examples/pytorch/translation)

---

## 9. Hugging Face Ekosistemi

- ğŸ¤– Transformers: Modeller
- âœ‚ï¸ Tokenizers: Metin parÃ§alama
- ğŸ“š Datasets: Veri kÃ¼mesi
- âš¡ Accelerate: EÄŸitim hÄ±zlandÄ±rma
- â˜ï¸ Hub: Model paylaÅŸÄ±m platformu

ğŸ”— [Hugging Face Hub](https://huggingface.co/models)

---

## 10. Zorluklar

- Dil Ã§eÅŸitliliÄŸi problemi (Ä°ngilizce dÄ±ÅŸÄ± modeller az)
- Uzun metinlerde self-attention maliyeti yÃ¼ksek
- Model kararlarÄ± aÃ§Ä±klanamÄ±yor (black-box)
- Ä°nternetten alÄ±nan verilerden gelen Ã¶nyargÄ± (bias)

---

## 11. BÃ¶lÃ¼m Ã–zeti

Bu bÃ¶lÃ¼mde:
- Transformer mimarisi, encoder-decoder yapÄ±sÄ±, attention, GPT ve BERT gibi modellerin doÄŸuÅŸu
- Hugging Face ile model kullanÄ±mÄ± ve NLP gÃ¶revleri
- Kod Ã¶rnekleri ve uygulamalar
gibi temel yapÄ± taÅŸlarÄ± Ã¶ÄŸrenildi.

ğŸ§­ Sonraki adÄ±m: Kendi veri kÃ¼menle model eÄŸitmek ve optimize etmek.
