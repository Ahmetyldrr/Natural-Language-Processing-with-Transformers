
# ğŸ“š BÃ¶lÃ¼m 1 â€“ Transformer'lara Merhaba (GeliÅŸtirilmiÅŸ Versiyon)

## 1. Transformerâ€™Ä±n DoÄŸuÅŸu

2017 yÄ±lÄ±nda Google tarafÄ±ndan yayÄ±mlanan â€œAttention Is All You Needâ€ makalesiyle, sÄ±ralÄ± modellerin yerini paralel Ã§alÄ±ÅŸan Transformer mimarisi aldÄ±. Bu mimari, NLP'de devrim yarattÄ±.

### ğŸ§  Ã–zellikler:
- RNN yerine tamamen paralel yapÄ±
- Self-attention ile baÄŸlam Ã¶ÄŸrenme
- Uzun dizilerde daha etkin performans

### ğŸ”— Kaynaklar:
- [Makale â€“ Attention Is All You Need (arXiv)](https://arxiv.org/abs/1706.03762)
- [Jay Alammar â€“ Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)
- [TÃ¼rkÃ§e: Transformer Mimarisi Nedir?](https://blog.openzeka.com/ai/transformer-modeli-nedir/)
- [Sebastian Ruder â€“ Transformer Family](https://sebastianruder.com/the-transformer-family/)

### ğŸ–¼ï¸ GÃ¶rseller:
![Transformer Timeline](https://jalammar.github.io/images/t/transformers-timeline.png)

---

## 2. GPT ve BERTâ€™in Ortaya Ã‡Ä±kÄ±ÅŸÄ±

GPT ve BERT, Transformer yapÄ±sÄ±nÄ±n farklÄ± bÃ¶lÃ¼mlerine (decoder ve encoder) odaklanan iki model ailesidir.

| Model | Katman | YÃ¶n | EÄŸitim | GÃ¶rev |
|-------|--------|-----|--------|-------|
| GPT   | Decoder | Tek yÃ¶nlÃ¼ | Language Modeling | Metin Ã¼retimi |
| BERT  | Encoder | Ã‡ift yÃ¶nlÃ¼ | Masked LM | Anlama ve sÄ±nÄ±flandÄ±rma |

### ğŸ”— Kaynaklar:
- [GPT Paper (OpenAI)](https://openai.com/research/language-unsupervised)
- [BERT Paper](https://arxiv.org/abs/1810.04805)
- [Blog: BERT vs GPT](https://towardsdatascience.com/bert-vs-gpt-comparing-their-strengths-634f5b40e49e)
- [TÃ¼rkÃ§e: Transformer Mimarisine GiriÅŸ](https://medium.com/@selencodes/transformer-mimarisi-8b8da595a581)

### ğŸ–¼ï¸ GÃ¶rseller:
![BERT vs GPT](https://jalammar.github.io/images/gpt2/gpt2-architecture.png)

---

## 3. Encoder-Decoder Framework

Encoder giriÅŸi temsile Ã§evirir, decoder bu temsilden Ã§Ä±ktÄ± Ã¼retir. Makine Ã§evirisi gibi gÃ¶revlerde kullanÄ±lÄ±r.

### ğŸ”— Kaynaklar:
- [Sequence to Sequence Models â€“ Colahâ€™s Blog](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)
- [Encoder-Decoder w/ Attention](https://machinelearningmastery.com/encoder-decoder-attention-sequence-to-sequence-prediction-keras/)
- [TÃ¼rkÃ§e: Seq2Seq Nedir?](https://www.veribilimiokulu.com/seq2seq-ve-attention-mekanizmasi/)

### ğŸ–¼ï¸ GÃ¶rseller:
![Encoder Decoder YapÄ±sÄ±](https://jalammar.github.io/images/seq2seq_2.png)

---

## 4. Attention MekanizmasÄ±

Attention, modelin her kelimeye farklÄ± dikkat vererek baÄŸlamsal karar vermesini saÄŸlar. Bu yapÄ± hizalama (alignment) iÃ§in de kullanÄ±lÄ±r.

### ğŸ”— Kaynaklar:
- [Bahdanau Attention (2014)](https://arxiv.org/abs/1409.0473)
- [Jay Alammar: Visual Attention](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)
- [TÃ¼rkÃ§e: Attention Nedir?](https://medium.com/@i_konak/transformer-mimarisi-tÃ¼rkÃ§e-anlatÄ±m-part-2-self-attention-ab5e5eec32f)

### ğŸ–¼ï¸ GÃ¶rseller:
![Attention](https://jalammar.github.io/images/transformer/transformer-self-attention.png)

---

## 5. Transformer Mimarisi

Transformer, Ã§ok katmanlÄ± self-attention + FFNN bloklarÄ±ndan oluÅŸur. EÄŸitim paralelleÅŸtirilebilir, uzun baÄŸlamlarla Ã§alÄ±ÅŸabilir.

### ğŸ”— Kaynaklar:
- [Google AI Blog](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html)
- [A Visual Guide to Transformers](https://e2eml.school/transformers.html)
- [TÃ¼rkÃ§e: Transformer Mimarisi â€“ Kodlama ZekasÄ±](https://kodlamazekasi.com/transformer-mimarisi-ve-attention/)

### ğŸ–¼ï¸ GÃ¶rseller:
![Transformer Architecture](https://jalammar.github.io/images/transformer/transformer.png)

---

## 6. Transfer Learning in NLP

ULMFiT ve BERT gibi modeller, bÃ¼yÃ¼k veriyle Ã¶n eÄŸitimden geÃ§irilip hedef gÃ¶reve Ã¶zel yeniden eÄŸitilir (fine-tuning).

### ğŸ”— Kaynaklar:
- [ULMFiT Paper](https://arxiv.org/abs/1801.06146)
- [Transfer Learning in NLP](https://ruder.io/nlp-imagenet/)
- [TÃ¼rkÃ§e: Transfer Ã–ÄŸrenme Nedir?](https://medium.com/@merveaydinlar/transfer-%C3%B6%C4%9Frenme-nedir-nas%C4%B1l-%C3%A7al%C4%B1%C5%9F%C4%B1r-485ba3d9c1e8)

---

## 7. Hugging Face Transformers

Hugging Face, farklÄ± gÃ¶revler iÃ§in hazÄ±r modeller sunar. PyTorch, TensorFlow desteklidir. Pipeline API Ã§ok pratiktir.

```python
from transformers import pipeline
classifier = pipeline("text-classification")
print(classifier("Transformers are awesome!"))
```

### ğŸ”— Kaynaklar:
- [Transformers GitHub](https://github.com/huggingface/transformers)
- [Hugging Face Docs](https://huggingface.co/docs/transformers/index)
- [TÃ¼rkÃ§e Hugging Face EÄŸitimi](https://github.com/firmai/machine-learning/blob/main/NLP/Huggingface-Turkce-Kaynaklar.md)

---

## 8. Uygulama Ã–rnekleri ve Kodlar

### a. Sentiment Analysis
```python
pipeline("text-classification")("I love this!")
```
â¡ï¸ [Text Classification Repo](https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-classification)

### b. Named Entity Recognition
```python
pipeline("ner", aggregation_strategy="simple")("Barack Obama was born in Hawaii.")
```
â¡ï¸ [NER Example](https://github.com/huggingface/transformers/tree/main/examples/pytorch/token-classification)

### c. Question Answering
```python
pipeline("question-answering")(
  question="Who founded Apple?", 
  context="Apple was founded by Steve Jobs and Steve Wozniak."
)
```
â¡ï¸ [QA Repo](https://github.com/huggingface/transformers/tree/main/examples/pytorch/question-answering)

---

## 9. Hugging Face Ekosistemi

- ğŸ¤– Transformers
- âœ‚ï¸ Tokenizers
- ğŸ“š Datasets
- âš¡ Accelerate
- â˜ï¸ Hugging Face Hub

ğŸ”— [Hugging Face Hub](https://huggingface.co/models)

---

## 10. Zorluklar

- ğŸ“‰ Veri dengesizliÄŸi ve dil Ã§eÅŸitliliÄŸi
- ğŸ§  AÃ§Ä±klanabilirlik zorluklarÄ±
- ğŸ’° EÄŸitim maliyetleri
- âš–ï¸ Ã–nyargÄ±lar (bias) ve etik sorunlar

---

## 11. Ã–zet

Bu bÃ¶lÃ¼mde:
- Transformer mimarisi ve avantajlarÄ±
- GPT ve BERT'in farklarÄ±
- Hugging Face ile uygulamalÄ± NLP
- Kodlar, gÃ¶rseller ve TÃ¼rkÃ§e kaynaklarla destekli kavramsal anlatÄ±m
Ã¶ÄŸrenildi.

â¡ï¸ Sonraki adÄ±m: Kendi veri setinle model fine-tune etmek!

