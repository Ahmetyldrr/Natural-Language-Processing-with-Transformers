
# ğŸ“˜ Chapter 1 â€“ Hello Transformers (AyrÄ±ntÄ±lÄ± AÃ§Ä±klama + Kod + Kaynak)

Bu bÃ¶lÃ¼m, Transformer mimarisinin ortaya Ã§Ä±kÄ±ÅŸÄ±nÄ±, neden Ã¶nemli olduÄŸunu, Ã¶nceki yÃ¶ntemlere gÃ¶re avantajlarÄ±nÄ± ve temel yapÄ± taÅŸlarÄ±nÄ± detaylÄ± bir ÅŸekilde ele almaktadÄ±r. AynÄ± zamanda Hugging Face ekosistemi ile bu teknolojileri nasÄ±l kolayca uygulayabileceÄŸimizi gÃ¶sterir.

---

## 1. Transformerâ€™Ä±n Ortaya Ã‡Ä±kÄ±ÅŸÄ±

2017 yÄ±lÄ±nda Google araÅŸtÄ±rmacÄ±larÄ± tarafÄ±ndan yayÄ±mlanan â€œAttention Is All You Needâ€ adlÄ± makale, RNN (Recurrent Neural Network) tabanlÄ± sÄ±ralÄ± veri modellemeye karÅŸÄ± bÃ¼yÃ¼k bir alternatif sundu.

### âœ”ï¸ Neden Ã¶nemli?
- EÄŸitim sÃ¼resi daha kÄ±sa
- ParalelleÅŸtirilebilir
- Uzun baÄŸlamlarÄ± daha iyi iÅŸler

### ğŸ§  Referanslar:
- https://arxiv.org/abs/1706.03762
- https://jalammar.github.io/illustrated-transformer/
- https://sebastianruder.com/the-transformer-family/

---

## 2. ULMFiT ve Transfer Ã–ÄŸrenmenin GÃ¼cÃ¼

ULMFiT, LSTM temelli bir model ile transfer Ã¶ÄŸrenmeyi NLP'ye taÅŸÄ±dÄ±. Bu, Transformer gibi modellerin daha az veriyle Ã§ok ÅŸey Ã¶ÄŸrenebilmesinin yolunu aÃ§tÄ±.

### ğŸ§  Kaynak:
- https://arxiv.org/abs/1801.06146
- https://docs.fast.ai/text.learner.html

---

## 3. GPT ve BERT'in DoÄŸuÅŸu

- **GPT:** Sadece decoder katmanÄ± ile Ã§alÄ±ÅŸÄ±r. Language modeling gÃ¶revine odaklanÄ±r.
- **BERT:** Sadece encoder katmanÄ± ile Ã§alÄ±ÅŸÄ±r. Masked Language Modeling kullanÄ±r.

### ğŸ§ª Basit Ã–rnek:
```python
from transformers import pipeline
generator = pipeline("text-generation", model="gpt2")
print(generator("Transformers are", max_length=20))
```

---

## 4. Encoder-Decoder YapÄ±sÄ±

Encoder giriÅŸi temsile Ã§evirir, decoder bu temsilden Ã§Ä±ktÄ± Ã¼retir.

### ğŸ§  Okuma:
- https://machinelearningmastery.com/encoder-decoder-attention-sequence-to-sequence-prediction-keras/
- https://colah.github.io/posts/2015-08-Understanding-LSTMs/

---

## 5. Attention MekanizmasÄ±

Attention, decoderâ€™Ä±n her giriÅŸ tokenâ€™Ä±na farklÄ± aÄŸÄ±rlÄ±k vermesini saÄŸlar. Bu sayede daha iyi Ã§eviri ve Ã¶zetleme yapÄ±lÄ±r.

### ğŸ§  Kaynaklar:
- https://arxiv.org/abs/1409.0473
- https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/

---

## 6. Transformer Mimarisi

Transformer mimarisi, katmanlÄ± self-attention ve feedforward yapÄ±larÄ±ndan oluÅŸur. Encoder ve decoder bloklarÄ± ayrÄ± ayrÄ± attention katmanlarÄ± iÃ§erir.

```python
from transformers import AutoTokenizer, AutoModel
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
model = AutoModel.from_pretrained("bert-base-uncased")
```

---

## 7. Hugging Face Transformers KÃ¼tÃ¼phanesi

Bu kÃ¼tÃ¼phane, model yÃ¼kleme, Ã¶n iÅŸleme, eÄŸitim ve ince ayar gibi iÅŸleri bÃ¼yÃ¼k Ã¶lÃ§Ã¼de kolaylaÅŸtÄ±rÄ±r.

### ğŸ“¦ Kurulum:
```bash
pip install transformers
```

### ğŸ‘‡ Text Classification Pipeline:
```python
from transformers import pipeline
classifier = pipeline("text-classification")
print(classifier("This product is awesome!"))
```

---

## 8. Uygulama Ã–rnekleri

### âœ… Named Entity Recognition (NER)
```python
ner = pipeline("ner", aggregation_strategy="simple")
ner("Barack Obama was born in Hawaii.")
```

### âœ… Soru Cevaplama
```python
qa = pipeline("question-answering")
qa(question="Who founded Apple?", context="Apple was founded by Steve Jobs and Steve Wozniak.")
```

### âœ… Ã–zetleme (Summarization)
```python
summarizer = pipeline("summarization")
summarizer("Transformers are the backbone of modern NLP.")
```

### âœ… Ã‡eviri (Translation)
```python
translator = pipeline("translation_en_to_de", model="Helsinki-NLP/opus-mt-en-de")
translator("Hello, how are you?")
```

---

## 9. Hugging Face Ekosistemi

### ğŸš€ Ana BileÅŸenler:
- **Transformers:** Modellerin kalbi
- **Tokenizers:** HÄ±zlÄ± tokenizasyon
- **Datasets:** BÃ¼yÃ¼k veri kÃ¼meleri
- **Accelerate:** EÄŸitim kolaylaÅŸtÄ±rÄ±cÄ±
- **Hub:** Modellerin ve veri kÃ¼melerinin merkezi

### ğŸ”— BaÄŸlantÄ±lar:
- https://huggingface.co/models
- https://huggingface.co/datasets
- https://huggingface.co/docs/transformers/index

---

## 10. Transformerâ€™larÄ±n ZorluklarÄ±

- ğŸ” Uzun dokÃ¼manlarda self-attention maliyetlidir
- ğŸ” AÃ§Ä±klanabilirlik problemi
- âš–ï¸ Ã–nyargÄ± (bias) riski
- ğŸŒ DÃ¼ÅŸÃ¼k kaynaklÄ± diller iÃ§in model eksikliÄŸi

---

## ğŸ”š SonuÃ§ ve Sonraki AdÄ±mlar

Transformer mimarisi, doÄŸal dil iÅŸleme alanÄ±nda ezber bozdu. Hugging Face ekosistemi sayesinde bu teknolojiler artÄ±k herkes iÃ§in ulaÅŸÄ±labilir. Bu bÃ¶lÃ¼mde teoriden pratiÄŸe geÃ§en temel konularÄ± iÅŸledik.

> ğŸ”œ Chapter 2â€™de metin sÄ±nÄ±flandÄ±rmayÄ± derinlemesine iÅŸleyeceÄŸiz!

