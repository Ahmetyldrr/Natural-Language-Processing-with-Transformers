
# ğŸ“˜ Chapter 1 â€“ Hello Transformers (Derinlemesine Teknik AÃ§Ä±klamalar + Kod + Kaynak)

Bu belge, Transformer mimarisinin temel yapÄ± taÅŸlarÄ±nÄ±, dikkat mekanizmasÄ±nÄ±, encoder-decoder yapÄ±sÄ±nÄ± ve Hugging Face ekosistemini kapsamlÄ± bir ÅŸekilde aÃ§Ä±klamaktadÄ±r. Ã–zellikle matematiksel ve yapÄ±sal bileÅŸenlere odaklanÄ±lmÄ±ÅŸtÄ±r.

---

## 1. Transformerâ€™Ä±n Ortaya Ã‡Ä±kÄ±ÅŸÄ±

Transformer modeli, RNN ve LSTM'in sÄ±ralÄ± doÄŸasÄ±ndan farklÄ± olarak tÃ¼m girdiyi aynÄ± anda iÅŸleyebilen self-attention mekanizmasÄ±na dayanÄ±r. Bu sayede paralel eÄŸitim, daha iyi baÄŸlam yakalama ve uzun dizilerle baÅŸa Ã§Ä±kma imkÃ¢nÄ± doÄŸar.

ğŸ“„ Makale: [Attention Is All You Need](https://arxiv.org/abs/1706.03762)

---

## 2. Encoder ve Matematiksel Temeli

Transformer mimarisinde **encoder**, giriÅŸ dizisini (Ã¶rneÄŸin bir cÃ¼mle) sabit boyutta temsillere (embedding'ler) Ã§evirir.

### âœ¨ Encoder'Ä±n BileÅŸenleri:

1. **Token Embedding**: Her kelime iÃ§in vektÃ¶r temsili.
2. **Positional Encoding**: Kelime sÄ±rasÄ±nÄ± modele kazandÄ±ran sabit trigonometrik fonksiyonlar.
3. **Multi-Head Self-Attention**: Her kelimenin diÄŸer tÃ¼m kelimelerle baÄŸÄ±nÄ± hesaba katmasÄ±.
4. **Feed Forward Network**: Ä°ki tam baÄŸlantÄ±lÄ± katmandan oluÅŸur.

### ğŸ“ Matematiksel FormÃ¼lasyon:

#### a. Self-Attention:
Her kelime embedding'i `x` iÃ§in 3 vektÃ¶r oluÅŸturulur:
- Query (Q), Key (K), Value (V)

Bu vektÃ¶rler bir aÄŸÄ±rlÄ±k matrisiyle Ã§arpÄ±lÄ±r:
```
Q = XW^Q,    K = XW^K,    V = XW^V
```

ArdÄ±ndan benzerlik hesaplanÄ±r:
```
Attention(Q, K, V) = softmax(QKáµ€ / âˆšd_k) V
```
Burada `d_k`, boyut sayÄ±sÄ±dÄ±r (Ã¶lÃ§eklendirme iÃ§in kullanÄ±lÄ±r).

#### b. Multi-Head Attention:
AynÄ± iÅŸlemi farklÄ± parametrelerle `h` kez uygular:
```
MultiHead(Q, K, V) = Concat(head_1, ..., head_h) W^O
```

#### c. Feed-Forward Network:
Ä°ki katmanlÄ± MLP:
```
FFN(x) = max(0, xWâ‚ + bâ‚)Wâ‚‚ + bâ‚‚
```

#### d. Residual + Layer Norm:
Her katman ÅŸu ÅŸekilde paketlenir:
```
LayerNorm(x + Sublayer(x))
```

---

## 3. Decoder NasÄ±l Ã‡alÄ±ÅŸÄ±r?

Decoder, encoder Ã§Ä±ktÄ±sÄ±nÄ± kullanarak sÄ±rayla yeni kelimeler Ã¼retir.

Ek olarak:
- **Masked Self-Attention**: Model gelecekteki kelimelere bakamaz (dil Ã¼retimi iÃ§in).
- Encoder-decoder attention katmanÄ± bulunur.

---

## 4. Positional Encoding DetayÄ±

Transformer dizideki sÄ±ralamayÄ± doÄŸal olarak anlamaz. Bu nedenle positional encoding (konum kodlamasÄ±) uygulanÄ±r:

```
PE(pos, 2i) = sin(pos / 10000^(2i/d_model))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))
```

Bu sayede model mutlak konum bilgisi edinir.

---

## 5. Hugging Face Transformers ile Uygulama

### Text Classification:
```python
from transformers import pipeline
classifier = pipeline("text-classification")
classifier("Transformers are amazing!")
```

### Soru-Cevap:
```python
qa = pipeline("question-answering")
qa(question="What is self-attention?", context="Self-attention allows a model to consider other words in the input.")
```

### NER:
```python
ner = pipeline("ner", aggregation_strategy="simple")
ner("Elon Musk founded SpaceX in California.")
```

---

## 6. EÄŸitim ZorluklarÄ±

- **Quadratic Attention**: Hesaplama karmaÅŸÄ±klÄ±ÄŸÄ± O(nÂ²), uzun dizilerde pahalÄ±dÄ±r.
- **Ã–nyargÄ±**: Modelin eÄŸitildiÄŸi veri kÃ¼mesindeki Ã¶nyargÄ±lar Ã§Ä±ktÄ±lara yansÄ±yabilir.
- **Yorumlanabilirlik**: Attention haritalarÄ± sÄ±nÄ±rlÄ± iÃ§gÃ¶rÃ¼ saÄŸlar.

---

## 7. GeliÅŸmiÅŸ Versiyonlar

- **BERT** (sadece encoder): Masked language modeling
- **GPT** (sadece decoder): Causal language modeling
- **T5/UL2**: Encoder-decoder tabanlÄ±, Ã§ok amaÃ§lÄ± modeller

---

## 8. Hugging Face Ekosistemi

- **Transformers**: Model eÄŸitimi ve Ã¶nceden eÄŸitilmiÅŸ modeller
- **Datasets**: 1000+ hazÄ±r NLP veri kÃ¼mesi
- **Tokenizers**: HÄ±zlÄ± ve Ã¶zelleÅŸtirilebilir tokenizerâ€™lar
- **Accelerate**: Kolay Ã§oklu GPU/TPU eÄŸitimi

ğŸ”— https://huggingface.co/docs

---

## ğŸ”š SonuÃ§

Transformer, NLPâ€™de devrim yaratmÄ±ÅŸtÄ±r. Hem teorik hem uygulamalÄ± olarak bu bÃ¶lÃ¼mde:
- Mimarinin temelleri
- Encoder-decoder yapÄ±sÄ±
- Attention matematiÄŸi
- Hugging Face ile uygulama Ã¶rnekleri

detaylÄ± olarak sunulmuÅŸtur.

