# Makine Çevirisi Hakkında Doğru/Yanlış Soruları ve Cevapları

Makine çevirisi artık insan seviyesini aşmıştır. (Doğru/Yanlış) Yanlış. Makine çevirisi en zor NLP (Doğal Dil İşleme) makine öğrenimi görevlerinden biridir. Makine çevirisi büyük veri kümeleri gerektirir. (Doğru/Yanlış) Doğru. Transformer modellerini aynı veri kümelerini kullanarak karşılaştırmaya gerek yoktur. (Doğru/Yanlış) Yanlış. Farklı modelleri karşılaştırmanın tek yolu aynı veri kümelerini kullanmaktır. BLEU, "mavi" anlamına gelen Fransızca bir kelime ve bir NLP metriğinin kısaltmasıdır. (Doğru/Yanlış) Doğru. BLEU, "Bilingual Evaluation Understudy Score" anlamına gelir ve hatırlanmasını kolaylaştırır. Düzeltme teknikleri BERT'i geliştirir. (Doğru/Yanlış) Doğru. Almanca-İngilizce çevirisi ile İngilizce-Almanca çevirisi makine çevirisi için aynı şeydir. (Doğru/Yanlış) Yanlış. Almanca'yı temsil edip başka bir dile çevirmek, İngilizce'yi temsil edip başka bir dile çevirmekle aynı süreç değildir. Dil yapıları aynı değildir.

# Orijinal Transformer Modelinin Yapısı

Orijinal Transformer çoklu-baş dikkat alt katmanı 2 başa sahiptir. (Doğru/Yanlış) Yanlış. Her dikkat alt katmanı 8 başa sahiptir. Orijinal Transformer kodlayıcısı 6 katmandan oluşur. (Doğru/Yanlış) Doğru. Orijinal Transformer kodlayıcısı 6 katmandan oluşur ancak sadece 2 çözücü katmanı vardır. (Doğru/Yanlış) Yanlış. 6 çözücü katmanı vardır.

# Transformer Eğitimleri

Transformer'ları çözücüler olmadan eğitebilirsiniz. (Doğru/Yanlış) Doğru. BERT'in mimarisi sadece kodlayıcılardan oluşur.

Bu çeviride herhangi bir Python kodu bulunmamaktadır. Eğer bir kod örneği isteseydiniz, seve seve yardımcı olurdum.

---

