# BERTViz ve LIT İle İlgili Doğru/Yanlış Soruları

Aşağıdaki çeviri, verilen İngilizce paragrafın birebir Türkçe çevirisidir.

BERTViz yalnızca BERT modelinin son katmanının çıktısını gösterir. (Doğru/Yanlış) Yanlış. BERTViz tüm katmanların çıktılarını gösterir. 
BERTViz, bir BERT modelinin her katmanının dikkat başlarını (attention heads) gösterir. (Doğru/Yanlış) Doğru. 
BERTViz, tokenlerin birbirleriyle nasıl ilişkili olduğunu gösterir. (Doğru/Yanlış) Doğru. 
LIT, BERTViz gibi dikkat başlarının (attention heads) iç işleyişini gösterir. (Doğru/Yanlış) Yanlış. 
Ancak, LIT non-probing tahminler yapar. Probing, bir algoritmanın dil temsillerini tahmin etme yoludur. (Doğru/Yanlış) Doğru. 
NER bir probing görevidir. (Doğru/Yanlış) Doğru. 
PCA ve UMAP non-probing görevlerdir. (Doğru/Yanlış) Doğru. 
LIME modele bağlı değildir (model-agnostic). (Doğru/Yanlış) Doğru. 
Transformers, tokenler arasındaki ilişkileri katman katman derinleştirir. (Doğru/Yanlış) Doğru. 
OpenAI Büyük Dil Modelleri (LLMs), LLMs'i bir dereceye kadar açıklayabilir (Doğru/Yanlış) Doğru. 
LLMs'i açıklamak hala zorlu bir görevdir.

Bu metinde Python kodları bulunmamaktadır, bu nedenle satır satır açıklama gerekmemektedir. Verilen metin doğrudan birebir çevrilmiştir.

---

